<h2>16年5月27</h2>
<h2>下午1：37</h2>
刚刚看完数据结构和算法，打算写一个小的搜索引擎。<br>
我现在想到的搜索引擎的几个模块，或者说步骤：<br>
&emsp;&emsp;1.网页爬虫，（BFS或是DFS）。<br>
&emsp;&emsp;2.抓取到的网页进行分析，提取网页中的关键字，关键字分词，建立倒排索引。<br>
&emsp;&emsp;3.对各个倒排索引文件进行归并。<br>
&emsp;&emsp;4.处理query。对倒排索引的缓存、关键字的交并差集计算、文档的得分排序<br>
<br>
补充细节:<br>
倒排索引的步骤：<br>
&emsp;&emsp;1.收集待建索引的文档。<br>
&emsp;&emsp;2.对这些文档中的文本进行词条化。<br>
&emsp;&emsp;3.对第2步产生的词条进行语言学预处理,得到词项。<br>
&emsp;&emsp;4.根据词项对所有文档建立索引。<br>
<br>
中文分词方法：分词的方法包括基于词典的最大匹配法(采用启发式规则来进行未定义词识别)和基于机器学习序列模型的方法(如隐马尔可夫模型或条件随机场模型)等,后者需要在手工切分好的语料上进行训练<br>
<br>
<h3>16年5月28</h3>
先从爬虫写起，做好一个爬虫，有了数据，在做后面的东西。。<br>
爬虫的流程：构造一个queue，初始化为几个URL种子，逐个去构造http请求，得到html数据。<br>
<img src="http://images.51cto.com/files/uploadimg/20101206/122752410.jpg"/><br>
爬虫的架构设计，网上找的，比看文字来的清晰简明。。<br>
计划用c++的libcurl来实现网页文件的获取。<br>
curl还是相当好使的，不想重复造轮子，网页抓取就靠它了！<br>
下面要实现的就是网页内容分析，取出其中的其他url。还有就是用bitmap判重url。<br>
其实url判重的方法有很多，比如在数据库中建立一个UNIQUE的字段，保证url不重复；用set或者hashset；用map存储等。。。<br>
但是上述方法都有一定的突出的劣势，尤其是遇到大数据量的时候。<br>
在数据库中存储，需要不停的操作数据库，磁盘IO次数太多，影响效率，而且数据库报错会导致程序终止。<br>
set、hashset占用的内存量太大。。。<br>
map判重也一样，占用内存太大。。<br>
所以我计划选用bitmap或是bloom filter来判重。<br>
还是得自己造轮子，感觉STL里面的hash,bitset都不好是，写了一个bloom_filter类，实现了bloom filter的功能<br>
类的定义在bloom_filter.h里，实现在bloom_filter.cpp里面。<br>
<br>
用了一个10亿大小的unsignedint的vector来作为bitmap使用，这样就可以有320亿个bit位用来存储hash值。<br>
用了11个不同的hash函数，都是用string的每个位来生成hash值的，返回值都是unsigned int值。<br>
插入一个字符串时，执行每一个hash函数，得到一个返回值hash_value，用hash_value/32，就得到了该结果对应的bitmap中哪一个unsigned int 数（vector中的第几个数），而hash_value%32就得到了在该数中对应的第几位bit（从地位到高位）。<br>
所以，bitmap中对应的数字与（0x01\<\<hash_value%32）进行或运算就得到了插入结果。<br>
即：bitmap[hash_value/32]|=(0x01\<\<hash_value%32)。<br>
这样的过程执行11遍。<br>
<br>
那么，检查某个字符串是不是已经存在，也是执行每一个hash函数，得到bitmap[hash_value]，这个值与（0x01\<\<hash_value%32）进行与运算，如果结果是1，则继续执行下一个hash函数，如果是0，则说明该字符串不存在，直接返回false。<br>
11个hash函数都存在，则返回true；<br>
<br>
为了程序的可移植性，不能用32，应该得到机器对unsigned int的存储位数，typeof(unsigned int)*8;<br>
<br>
在html文档中提取url，用boost库提供的regex正则匹配，<br>
但是写或者找一个url的好一点的正则表达式，竟然弄了半个小时了，，，holy fuck ...<br>
原来正则表达式这么不好写。。<br>
还是自己动手丰衣足食啊，最后还是自己写了一个：<br>
string pattern="https?://[a-zA-Z_0-9/\\.]+\\\\.(com\\.cn|net\\\\.cn|com|cn|net|org|biz|info|cc|tv|html|php|jsp|aspx)(/[a-zA-Z_0-9]+/?)*(\\\\?)?([a-zA-Z_0-9]+=[a-zA-Z_0-9&]|[a-zA-Z_0-9&])*";<br>
能匹配大部分的完整url，但是还有一些不能匹配，比如url中有中文编码的等等。。。<br>
而且匹配出来的很多东西都不是网页url地址，有的是下载文件的地址，有的根本就进去不（404，403）,有的是GET信息填写不全，访问url会自动跳转。<br>
所以还是得仔细考虑考虑，写的全面点，，，<br>
这么个小东西居然卡住了，，，<br>
终于搞定了正则表达式了，最后定稿的表达式为：<br>
string pattern="http://(?!s[0-9]\\.)[a-zA-Z_0-9/\\.]+\\.(com\\.cn|net\\.cn|com|cn|net|org|biz|info|cc|tv)((/[a-zA-Z_0-9]+)*/?\\.(html|php|jsp|aspx))?";<br>
这样得到的url就是没有GET请求信息的，只包含根目录，子目录，和各种页面文件的url。<br>
<br>
现在出现的问题竟然是curl不能请求子目录的页面，也就是www.baidu.com可以正常访问，但是，www.baidu.com/index.html(假设的页面)这样的结构就不能访问。<br>这是什么问题呢。。。<br>
终于找到了问题的原因，问题不再curl里，在我生成的存储文件名，文件名中不能有/。。<br>
我换了一种文件名表示方式，用域名和url的hash值组成文件名。。<br>
例如image.baidu.com+hash(url)。<br>
<br>
爬取网页的策略要考虑一下，目前用的是BFS，但是感觉爬出的来的东西太杂了。<br>
用DFS还是BFS要考虑一下。。。<br>
还有网页的url相似度，网页的主题判别等等。。。<br>
<br>
现在爬虫的效率也很低，要改进一下。<br>
改进了一下磁盘IO的频率，爬虫快多了。<br>
现在要做的最重要的事是聚焦爬虫！爬取特定主题的内容页面。<br>
聚焦爬虫的算法，我查到的有两种：Timed-PageRank算法，Widrow-Hoff分类器。<br>
Timed-PageRank算法就是用网页的新颖度和内容的相关度改进了PageRank算法。<br>
Widrow-Hoff，这是个机器学习领域用的比较多的算法，神经网络之类的，主要是训练一部分主题相关的样本，然后用新页面与中心向量进行计算比较，超过阈值就“相关”！<br>

<br>
其实上面这两个都是相关度的计算，就是已经爬取到了页面，对该页面的计算，那么在没有爬取之前，也可以对URL进行相关度的预测。<br>
发现了一个新算法，感觉把这两个方面结合起来了，就是BLCT算法。<br>
<a href="https://www.google.com.hk/url?sa=t&rct=j&q=&esrc=s&source=web&cd=10&ved=0ahUKEwjlx5D79IHNAhWHsI8KHT4VBYMQFghNMAk&url=%68%74%74%70%3a%2f%2f%77%77%77%2e%61%72%6f%63%6d%61%67%2e%63%6f%6d%2f%67%65%74%61%72%74%69%63%6c%65%2f%3f%61%69%64%3d%33%38%36%38%36%38%36%63%36%66%39%61%63%63%62%30&usg=AFQjCNH0Eu9ldlnZbXiTDOrYeXhktLq50A">BLCT相关论文</a><br>
BLCT算法与Timed-PageRank算法对于入链对本页面的相似度的影响（PageRank的阻尼系数）的处理的不同的，不同点在于：BLCT算法是用父节点的内容相似度与本节点的内容相似度来确定该链接的相似度，而Timed-PageRank是把本节点的时间系数。<br>
但是我有一点疑惑，既然BLCT算法中，用本页面的内容相似度来计算，那么一定是把该页面已经爬取下来了，那还怎么处理呢？！难道是超过阈值（主题相关）的页面就提取里面的url，然后为该页面建索引，而低于阈值（主题不相关）就直接放弃么？<br>
还要在考虑考虑，不能老依赖论文，也要自己想！<br>
