<h2>16年5月27</h2>
<h2>下午1：37</h2>
刚刚看完数据结构和算法，打算写一个小的搜索引擎。<br>
我现在想到的搜索引擎的几个模块，或者说步骤：<br>
&emsp;&emsp;1.网页爬虫，（BFS或是DFS）。<br>
&emsp;&emsp;2.抓取到的网页进行分析，提取网页中的关键字，关键字分词，建立倒排索引。<br>
&emsp;&emsp;3.对各个倒排索引文件进行归并。<br>
&emsp;&emsp;4.处理query。对倒排索引的缓存、关键字的交并差集计算、文档的得分排序<br>
<br>
补充细节:<br>
倒排索引的步骤：<br>
&emsp;&emsp;1.收集待建索引的文档。<br>
&emsp;&emsp;2.对这些文档中的文本进行词条化。<br>
&emsp;&emsp;3.对第2步产生的词条进行语言学预处理,得到词项。<br>
&emsp;&emsp;4.根据词项对所有文档建立索引。<br>
<br>
中文分词方法：分词的方法包括基于词典的最大匹配法(采用启发式规则来进行未定义词识别)和基于机器学习序列模型的方法(如隐马尔可夫模型或条件随机场模型)等,后者需要在手工切分好的语料上进行训练<br>
<br>
<h3>16年5月28</h3>
先从爬虫写起，做好一个爬虫，有了数据，在做后面的东西。。<br>
爬虫的流程：构造一个queue，初始化为几个URL种子，逐个去构造http请求，得到html数据。<br>
<img src="http://images.51cto.com/files/uploadimg/20101206/122752410.jpg"/><br>
爬虫的架构设计，网上找的，比看文字来的清晰简明。。<br>
计划用c++的libcurl来实现网页文件的获取。<br>
curl还是相当好使的，不想重复造轮子，网页抓取就靠它了！<br>
下面要实现的就是网页内容分析，取出其中的其他url。还有就是用bitmap判重url。<br>
其实url判重的方法有很多，比如在数据库中建立一个UNIQUE的字段，保证url不重复；用set或者hashset；用map存储等。。。<br>
但是上述方法都有一定的突出的劣势，尤其是遇到大数据量的时候。<br>
在数据库中存储，需要不停的操作数据库，磁盘IO次数太多，影响效率，而且数据库报错会导致程序终止。<br>
set、hashset占用的内存量太大。。。<br>
map判重也一样，占用内存太大。。<br>
所以我计划选用bitmap或是bloom filter来判重。<br>
