<h2>16年5月27</h2>
<h2>下午1：37</h2>
刚刚看完数据结构和算法，打算写一个小的搜索引擎。<br>
我现在想到的搜索引擎的几个模块，或者说步骤：<br>
&emsp;&emsp;1.网页爬虫，（BFS或是DFS）。<br>
&emsp;&emsp;2.抓取到的网页进行分析，提取网页中的关键字，关键字分词，建立倒排索引。<br>
&emsp;&emsp;3.对各个倒排索引文件进行归并。<br>
&emsp;&emsp;4.处理query。对倒排索引的缓存、关键字的交并差集计算、文档的得分排序<br>
<br>
补充细节:<br>
倒排索引的步骤：<br>
&emsp;&emsp;1.收集待建索引的文档。<br>
&emsp;&emsp;2.对这些文档中的文本进行词条化。<br>
&emsp;&emsp;3.对第2步产生的词条进行语言学预处理,得到词项。<br>
&emsp;&emsp;4.根据词项对所有文档建立索引。<br>
<br>
中文分词方法：分词的方法包括基于词典的最大匹配法(采用启发式规则来进行未定义词识别)和基于机器学习序列模型的方法(如隐马尔可夫模型或条件随机场模型)等,后者需要在手工切分好的语料上进行训练<br>
<br>
<h3>16年5月28</h3>
先从爬虫写起，做好一个爬虫，有了数据，在做后面的东西。。<br>
爬虫的流程：构造一个queue，初始化为几个URL种子，逐个去构造http请求，得到html数据。<br>
<img src="http://images.51cto.com/files/uploadimg/20101206/122752410.jpg"/><br>
爬虫的架构设计，网上找的，比看文字来的清晰简明。。<br>
计划用c++的libcurl来实现网页文件的获取。<br>
curl还是相当好使的，不想重复造轮子，网页抓取就靠它了！<br>
下面要实现的就是网页内容分析，取出其中的其他url。还有就是用bitmap判重url。<br>
其实url判重的方法有很多，比如在数据库中建立一个UNIQUE的字段，保证url不重复；用set或者hashset；用map存储等。。。<br>
但是上述方法都有一定的突出的劣势，尤其是遇到大数据量的时候。<br>
在数据库中存储，需要不停的操作数据库，磁盘IO次数太多，影响效率，而且数据库报错会导致程序终止。<br>
set、hashset占用的内存量太大。。。<br>
map判重也一样，占用内存太大。。<br>
所以我计划选用bitmap或是bloom filter来判重。<br>
还是得自己造轮子，感觉STL里面的hash,bitset都不好是，写了一个bloom_filter类，实现了bloom filter的功能<br>
类的定义在bloom_filter.h里，实现在bloom_filter.cpp里面。<br>
<br>
用了一个10亿大小的unsignedint的vector来作为bitmap使用，这样就可以有320亿个bit位用来存储hash值。<br>
用了11个不同的hash函数，都是用string的每个位来生成hash值的，返回值都是unsigned int值。<br>
插入一个字符串时，执行每一个hash函数，得到一个返回值hash_value，用hash_value/32，就得到了该结果对应的bitmap中哪一个unsigned int 数（vector中的第几个数），而hash_value%32就得到了在该数中对应的第几位bit（从地位到高位）。<br>
所以，bitmap中对应的数字与（0x01\<\<hash_value%32）进行或运算就得到了插入结果。<br>
即：bitmap[hash_value/32]|=(0x01\<\<hash_value%32)。<br>
这样的过程执行11遍。<br>
<br>
那么，检查某个字符串是不是已经存在，也是执行每一个hash函数，得到bitmap[hash_value]，这个值与（0x01\<\<hash_value%32）进行与运算，如果结果是1，则继续执行下一个hash函数，如果是0，则说明该字符串不存在，直接返回false。<br>
11个hash函数都存在，则返回true；<br>
