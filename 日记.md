<h2>16年5月27</h2>
<h2>下午1：37</h2>
刚刚看完数据结构和算法，打算写一个小的搜索引擎。<br>
我现在想到的搜索引擎的几个模块，或者说步骤：<br>
&emsp;&emsp;1.网页爬虫，（BFS或是DFS）。<br>
&emsp;&emsp;2.抓取到的网页进行分析，提取网页中的关键字，关键字分词，建立倒排索引。<br>
&emsp;&emsp;3.对各个倒排索引文件进行归并。<br>
&emsp;&emsp;4.处理query。对倒排索引的缓存、关键字的交并差集计算、文档的得分排序<br>
<br>
补充细节:<br>
倒排索引的步骤：<br>
&emsp;&emsp;1.收集待建索引的文档。<br>
&emsp;&emsp;2.对这些文档中的文本进行词条化。<br>
&emsp;&emsp;3.对第2步产生的词条进行语言学预处理,得到词项。<br>
&emsp;&emsp;4.根据词项对所有文档建立索引。<br>
<br>
中文分词方法：分词的方法包括基于词典的最大匹配法(采用启发式规则来进行未定义词识别)和基于机器学习序列模型的方法(如隐马尔可夫模型或条件随机场模型)等,后者需要在手工切分好的语料上进行训练<br>
<br>
<h3>16年5月28</h3>
先从爬虫写起，做好一个爬虫，有了数据，在做后面的东西。。<br>
爬虫的流程：构造一个queue，初始化为几个URL种子，逐个去构造http请求，得到html数据。<br>
<img src="http://images.51cto.com/files/uploadimg/20101206/122752410.jpg"/><br>
爬虫的架构设计，网上找的，比看文字来的清晰简明。。<br>
计划用c++的libcurl来实现网页文件的获取。<br>
curl还是相当好使的，不想重复造轮子，网页抓取就靠它了！<br>
下面要实现的就是网页内容分析，取出其中的其他url。还有就是用bitmap判重url。<br>
其实url判重的方法有很多，比如在数据库中建立一个UNIQUE的字段，保证url不重复；用set或者hashset；用map存储等。。。<br>
但是上述方法都有一定的突出的劣势，尤其是遇到大数据量的时候。<br>
在数据库中存储，需要不停的操作数据库，磁盘IO次数太多，影响效率，而且数据库报错会导致程序终止。<br>
set、hashset占用的内存量太大。。。<br>
map判重也一样，占用内存太大。。<br>
所以我计划选用bitmap或是bloom filter来判重。<br>
还是得自己造轮子，感觉STL里面的hash,bitset都不好是，写了一个bloom_filter类，实现了bloom filter的功能<br>
类的定义在bloom_filter.h里，实现在bloom_filter.cpp里面。<br>
<br>
用了一个10亿大小的unsignedint的vector来作为bitmap使用，这样就可以有320亿个bit位用来存储hash值。<br>
用了11个不同的hash函数，都是用string的每个位来生成hash值的，返回值都是unsigned int值。<br>
插入一个字符串时，执行每一个hash函数，得到一个返回值hash_value，用hash_value/32，就得到了该结果对应的bitmap中哪一个unsigned int 数（vector中的第几个数），而hash_value%32就得到了在该数中对应的第几位bit（从地位到高位）。<br>
所以，bitmap中对应的数字与（0x01\<\<hash_value%32）进行或运算就得到了插入结果。<br>
即：bitmap[hash_value/32]|=(0x01\<\<hash_value%32)。<br>
这样的过程执行11遍。<br>
<br>
那么，检查某个字符串是不是已经存在，也是执行每一个hash函数，得到bitmap[hash_value]，这个值与（0x01\<\<hash_value%32）进行与运算，如果结果是1，则继续执行下一个hash函数，如果是0，则说明该字符串不存在，直接返回false。<br>
11个hash函数都存在，则返回true；<br>
<br>
为了程序的可移植性，不能用32，应该得到机器对unsigned int的存储位数，typeof(unsigned int)*8;<br>
<br>
在html文档中提取url，用boost库提供的regex正则匹配，<br>
但是写或者找一个url的好一点的正则表达式，竟然弄了半个小时了，，，holy fuck ...<br>
原来正则表达式这么不好写。。<br>
还是自己动手丰衣足食啊，最后还是自己写了一个：<br>
string pattern="https?://[a-zA-Z_0-9/\\.]+\\\\.(com\\.cn|net\\\\.cn|com|cn|net|org|biz|info|cc|tv|html|php|jsp|aspx)(/[a-zA-Z_0-9]+/?)*(\\\\?)?([a-zA-Z_0-9]+=[a-zA-Z_0-9&]|[a-zA-Z_0-9&])*";<br>
能匹配大部分的完整url，但是还有一些不能匹配，比如url中有中文编码的等等。。。<br>
而且匹配出来的很多东西都不是网页url地址，有的是下载文件的地址，有的根本就进去不（404，403）,有的是GET信息填写不全，访问url会自动跳转。<br>
所以还是得仔细考虑考虑，写的全面点，，，<br>
这么个小东西居然卡住了，，，<br>
终于搞定了正则表达式了，最后定稿的表达式为：<br>
string pattern="http://(?!s[0-9]\\.)[a-zA-Z_0-9/\\.]+\\.(com\\.cn|net\\.cn|com|cn|net|org|biz|info|cc|tv)((/[a-zA-Z_0-9]+)*/?\\.(html|php|jsp|aspx))?";<br>
这样得到的url就是没有GET请求信息的，只包含根目录，子目录，和各种页面文件的url。<br>
<br>
现在出现的问题竟然是curl不能请求子目录的页面，也就是www.baidu.com可以正常访问，但是，www.baidu.com/index.html(假设的页面)这样的结构就不能访问。<br>这是什么问题呢。。。<br>
终于找到了问题的原因，问题不再curl里，在我生成的存储文件名，文件名中不能有/。。<br>
我换了一种文件名表示方式，用域名和url的hash值组成文件名。。<br>
例如image.baidu.com+hash(url)。<br>
<br>
爬取网页的策略要考虑一下，目前用的是BFS，但是感觉爬出的来的东西太杂了。<br>
用DFS还是BFS要考虑一下。。。<br>
还有网页的url相似度，网页的主题判别等等。。。<br>
<br>
现在爬虫的效率也很低，要改进一下。<br>
改进了一下磁盘IO的频率，爬虫快多了。<br>
现在要做的最重要的事是聚焦爬虫！爬取特定主题的内容页面。<br>
聚焦爬虫的算法，我查到的有两种：Timed-PageRank算法，Widrow-Hoff分类器。<br>
Timed-PageRank算法就是用网页的新颖度和内容的相关度改进了PageRank算法。<br>
Widrow-Hoff，这是个机器学习领域用的比较多的算法，神经网络之类的，主要是训练一部分主题相关的样本，然后用新页面与中心向量进行计算比较，超过阈值就“相关”！<br>

<br>
其实上面这两个都是相关度的计算，就是已经爬取到了页面，对该页面的计算，那么在没有爬取之前，也可以对URL进行相关度的预测。<br>
发现了一个新算法，感觉把这两个方面结合起来了，就是BLCT算法。<br>
<a href="https://www.google.com.hk/url?sa=t&rct=j&q=&esrc=s&source=web&cd=10&ved=0ahUKEwjlx5D79IHNAhWHsI8KHT4VBYMQFghNMAk&url=%68%74%74%70%3a%2f%2f%77%77%77%2e%61%72%6f%63%6d%61%67%2e%63%6f%6d%2f%67%65%74%61%72%74%69%63%6c%65%2f%3f%61%69%64%3d%33%38%36%38%36%38%36%63%36%66%39%61%63%63%62%30&usg=AFQjCNH0Eu9ldlnZbXiTDOrYeXhktLq50A">BLCT相关论文</a><br>
